{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec3e85c",
   "metadata": {},
   "source": [
    "拟合模型的2个目标：优化（train dataset） +泛化（beyond it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34820b1",
   "metadata": {},
   "source": [
    "导数 梯度 链式法则 \n",
    "# 求导重要---用于优化损失函数 必须选可微的Lossfunction\n",
    "\n",
    "# 用matplotlib 可视化(作图\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ece12",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    "设计好模型，系统会构建一个计算图computational graph？ 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 \n",
    "自动微分使系统能够随后反向传播梯度backpropagate\n",
    "backpropagate：跟踪整个计算图，填充关于每个参数的偏导数\n",
    "gradient: note that 关于向量x的梯度是向量，并且与x具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe37c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e8c87",
   "metadata": {},
   "source": [
    "# 启动启用自动求导机制 开始跟踪该张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff0a9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(4.0,requires_grad=True) \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84fd695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=2 * torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cc3e5",
   "metadata": {},
   "source": [
    "调用反向传播函数来自动计算y关于x每个分量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73d5c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad\n",
    "#y对x 的梯度 用指示x来显示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e1174a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4*x  #验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f2c0dd",
   "metadata": {},
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccfc4fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b18974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = x.sum()\n",
    "y2.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1c292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#没懂参数？？？？？？？？？\n",
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddf04d",
   "metadata": {},
   "source": [
    "# 将计算移出计算图 （直接保留此处的结果为常数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f601f5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y=x*x\n",
    "stayc=y.detach()\n",
    "z=stayc *x\n",
    "\n",
    "z.sum().backward() #向量的求导一般求个sum（变标量） 再求导\n",
    "x.grad == stayc #验证 u为常数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b1740",
   "metadata": {},
   "source": [
    "反向累积 反向传递计算（去想图） 其实就是手算复合导数的顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe8519cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。\n",
    "def f(a):\n",
    "    b=a*2;\n",
    "    while b.norm()<1000:\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c=b\n",
    "    else:\n",
    "        c=100*b\n",
    "    return c\n",
    "\n",
    "t=torch.randn(size=(),requires_grad=True)\n",
    "e=f(t)\n",
    "e.backward()\n",
    "t.grad == e/t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80052c",
   "metadata": {},
   "source": [
    "# 分母布局 分子布局\n",
    "梯度结果 行数与分母相同 则是分母布局demoninator layout\n",
    "eg f(x)对向量x求梯度 结构是x维度相同的 列向量\n",
    "\n",
    "如果结果得行向量（1×m) 则一行 与f对x求导，（f/x)的分子相同 则是分子布局numenator\n",
    "\n",
    "二者互为转置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28f85e",
   "metadata": {},
   "source": [
    "# y是标量方程 \n",
    "对向量导  则是普通函数 梯度为向量\n",
    "# y是向量方程 \n",
    "向量y 关于 向量x 的导数是一个矩阵\n",
    "y(x) y也是列向量\n",
    "若是分母布局 则结果要和向量x为同 行数确定！ 于是每行 对f展开 列数与f维度相同\n",
    "若是分子布局 则结果行数==f维度 每行 fm分别对x展开\n",
    "\n",
    "# 向量只是标量的一种装填表示方式而已\n",
    "\n",
    "TB：\n",
    "ay导y 为a转置:\n",
    "A mv y 对y导 结果是A.T \n",
    "\n",
    "xTA A:\n",
    "xTAx 导 Ax+ATx\n",
    "\n",
    "uv（都标量） 导x ==u(v导x)+v(u导x)\n",
    "\n",
    "<u,v> 导x (都是向量）\n",
    "=u(v导x)+v(u导x) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96002071",
   "metadata": {},
   "source": [
    "亚导数 将函数不可导的点进行补充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46e98d",
   "metadata": {},
   "source": [
    "# 正向求导耗时（要从头到尾扫一遍） 省空间 \n",
    "# 反向求导耗空间（要存每一步的导）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
